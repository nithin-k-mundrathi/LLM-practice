{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrlerzGN+9j3YWFC13P0/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithin-k-mundrathi/LLM-practice/blob/main/RAG/RAG_C_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Retriver"
      ],
      "metadata": {
        "id": "AWTiamUZl4uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py\"\n",
        "output_file = \"grequests.py\"\n",
        "\n",
        "# Prepare the curl command\n",
        "curl_command = [\n",
        "    \"curl\",\n",
        "    \"-o\", output_file,\n",
        "    url\n",
        "]\n",
        "\n",
        "# Execute the curl command\n",
        "try:\n",
        "    subprocess.run(curl_command, check=True)\n",
        "    print(\"Download successful.\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"Failed to download the file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWoU1vjKl6Rc",
        "outputId": "e8d8dba7-832c-482b-86b7-64f4f907a396"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.32.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BF48Eqsl8g6",
        "outputId": "af483878-2d44-46d7-ce67-9e4ed4acc1dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install beautifulsoup4==4.12.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B0LzbITmAqI",
        "outputId": "1cae50c1-2e01-4189-c645-12190302c7e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.12.3) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fecth the Data"
      ],
      "metadata": {
        "id": "IIYb5cgDmFdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# URLs of the Wikipedia articles mapped to keywords\n",
        "urls = {\n",
        "    \"prompt engineering\": \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n",
        "    \"artificial intelligence\":\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "    \"llm\": \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
        "    \"llms\": \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
        "}"
      ],
      "metadata": {
        "id": "92h9zkVamCIr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the Data"
      ],
      "metadata": {
        "id": "E6Ywo50EmHDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_clean(url):\n",
        "    # Fetch the content of the URL\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the main content of the article, ignoring side boxes and headers\n",
        "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
        "\n",
        "    # Remove less relevant sections such as \"See also\", \"References\", etc.\n",
        "    for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n",
        "        section = content.find('span', {'id': section_title})\n",
        "        if section:\n",
        "            for sib in section.parent.find_next_siblings():\n",
        "                sib.decompose()\n",
        "            section.parent.decompose()\n",
        "\n",
        "    # Focus on extracting and cleaning text from paragraph tags only\n",
        "    paragraphs = content.find_all('p')\n",
        "    cleaned_text = ' '.join(paragraph.get_text(separator=' ', strip=True) for paragraph in paragraphs)\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)  # Remove citation markers like [1], [2], etc.\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "ef4-2VT8mDkX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retieval Process for User Input"
      ],
      "metadata": {
        "id": "QzTs28RFmSWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def process_query(user_input, num_words):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Check for any of the specified keywords in the input\n",
        "    matched_keyword = next((keyword for keyword in urls if keyword in user_input), None)\n",
        "\n",
        "    if matched_keyword:\n",
        "        print(f\"Fetching data from: {urls[matched_keyword]}\")\n",
        "        cleaned_text = fetch_and_clean(urls[matched_keyword])\n",
        "\n",
        "        # Limit the display to the specified number of words from the cleaned text\n",
        "        words = cleaned_text.split()  # Split the text into words\n",
        "        first_n_words = ' '.join(words[:num_words])  # Join the first n words into a single string\n",
        "\n",
        "        # Wrap the first n words to 80 characters wide for display\n",
        "        wrapped_text = textwrap.fill(first_n_words, width=80)\n",
        "        print(\"\\nFirst {} words of the cleaned text:\".format(num_words))\n",
        "        print(wrapped_text)  # Print the first n words as a well-formatted paragraph\n",
        "\n",
        "        # Use the exact same first_n_words for the GPT-4 prompt to ensure consistency\n",
        "        prompt = f\"Summarize the following information about {matched_keyword}:\\n{first_n_words}\"\n",
        "        wrapped_prompt = textwrap.fill(prompt, width=80)  # Wrap prompt text\n",
        "        print(\"\\nPrompt for Generator:\", wrapped_prompt)\n",
        "\n",
        "        # Return the specified number of words\n",
        "        return first_n_words\n",
        "    else:\n",
        "        print(\"No relevant keywords found. Please enter a query related to 'LLM', 'LLMs', or 'Prompt Engineering'.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "8W5Xy3uEmKYX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generator"
      ],
      "metadata": {
        "id": "Vow4WDpomgbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.Adaptive RAG Selection Based on Human Ranking: Integrating HF-RAG for Augmented Document Inputs"
      ],
      "metadata": {
        "id": "LeEanekUmi-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dowload image\n",
        "from grequests import download\n",
        "\n",
        "# Define your variables\n",
        "directory = \"/content/RAG-Driven-Generative-AI/Chapter05\"\n",
        "filename = \"rag_strategy.png\"\n",
        "download(directory, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCbaVqWCmfGs",
        "outputId": "9d2b3738-1e04-4db1-bcfc-ec24b66eb27c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'rag_strategy.png' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title show image\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Specify the path to your PNG file\n",
        "image_path = '/content/rag_strategy.png'\n",
        "\n",
        "# Display the image\n",
        "# Display the image with specified width and height\n",
        "display(Image(filename=image_path, width=500, height=400))  # Adjust the width and height as needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "8MzqIAOEmpOv",
        "outputId": "0116fa56-a3b6-49bd-dcc3-1004cf61c5b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "PGEgaHJlZj0iL0RlbmlzMjA1NC9SQUctRHJpdmVuLUdlbmVyYXRpdmUtQUkvbWFpbi9jb250ZW50L1JBRy1Ecml2ZW4tR2VuZXJhdGl2ZS1BSS9DaGFwdGVyMDUvcmFnX3N0cmF0ZWd5LnBuZyI+TW92ZWQgUGVybWFuZW50bHk8L2E+LgoK\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 500,
              "height": 400
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Request user input for keyword parsing\n",
        "user_input = input(\"Enter your query: \").lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STcR6gIOmsCE",
        "outputId": "75f38e16-72ca-46a5-fa1f-56c9b52ea41d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query: what is an LLM?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Mean Ranking Simulation Scenario"
      ],
      "metadata": {
        "id": "f0NEyU1dnFJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select a score between 1 and 5 to run the simulation\n",
        "ranking=5"
      ],
      "metadata": {
        "id": "Oc6wl-m0nAvy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# initializing the text for the generative AI model simulations\n",
        "text_input=[]"
      ],
      "metadata": {
        "id": "xeRRWFKCnJT0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking -1-2- No RAG"
      ],
      "metadata": {
        "id": "JuoFHs5znK8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ranking>=1 and ranking<3:\n",
        "  text_input=user_input"
      ],
      "metadata": {
        "id": "ELeH1wd0nKPF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking 3-4 : Human Expert Feedback RAG only"
      ],
      "metadata": {
        "id": "5ZjPmcdDnPIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf=False\n",
        "if ranking>=3 and ranking<5:\n",
        "  hf=True"
      ],
      "metadata": {
        "id": "FMJr6kNLnOeW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if hf==True:\n",
        "  from grequests import download\n",
        "  directory = \"Chapter05\"\n",
        "  filename = \"human_feedback.txt\"\n",
        "  download(directory, filename)"
      ],
      "metadata": {
        "id": "aIyzVV08nUO5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if hf==True:\n",
        "  # Check if 'human_feedback.txt' exists\n",
        "    efile = os.path.exists('/content/human_feedback.txt')\n",
        "\n",
        "    if efile:\n",
        "        # Read and clean the file content\n",
        "        with open('/content/human_feedback.txt', 'r') as file:\n",
        "            content = file.read().replace('\\n', ' ').replace('#', '')  # Removing new line and markdown characters\n",
        "            #print(content)  # Uncomment for debugging or maintenance display\n",
        "        text_input=content\n",
        "        print(text_input)\n",
        "    else:\n",
        "      print(\"File not found\")\n",
        "      hf=False"
      ],
      "metadata": {
        "id": "2Smpy5-Nnc7i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking 5 : RAG only with no HF"
      ],
      "metadata": {
        "id": "WcLgP9R_nqG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ranking>=5:\n",
        "  max_words=100 #Limit: the size of the data we can add to the input\n",
        "  rdata=process_query(user_input,max_words)\n",
        "  print(rdata) # for maintenance if necessary\n",
        "  if rdata:\n",
        "        rdata_clean = rdata.replace('\\n', ' ').replace('#', '')\n",
        "        rdata_sentences = rdata_clean.split('. ')\n",
        "  text_input=rdata\n",
        "  print(text_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFNwIzrUnjZj",
        "outputId": "a8b0215b-60c7-49be-8cda-49306ed7e530"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from: https://en.wikipedia.org/wiki/Large_language_model\n",
            "\n",
            "First 100 words of the cleaned text:\n",
            "A large language model ( LLM ) is a type of machine learning model designed for\n",
            "natural language processing tasks such as language generation . LLMs are\n",
            "language models with many parameters, and are trained with self-supervised\n",
            "learning on a vast amount of text. The largest and most capable LLMs are\n",
            "generative pretrained transformers (GPTs). Modern models can be fine-tuned for\n",
            "specific tasks or guided by prompt engineering . [ ] These models acquire\n",
            "predictive power regarding syntax , semantics , and ontologies [ ] inherent in\n",
            "human language corpora, but they also inherit inaccuracies and biases present in\n",
            "the\n",
            "\n",
            "Prompt for Generator: Summarize the following information about llm: A large language model ( LLM ) is\n",
            "a type of machine learning model designed for natural language processing tasks\n",
            "such as language generation . LLMs are language models with many parameters, and\n",
            "are trained with self-supervised learning on a vast amount of text. The largest\n",
            "and most capable LLMs are generative pretrained transformers (GPTs). Modern\n",
            "models can be fine-tuned for specific tasks or guided by prompt engineering . [\n",
            "] These models acquire predictive power regarding syntax , semantics , and\n",
            "ontologies [ ] inherent in human language corpora, but they also inherit\n",
            "inaccuracies and biases present in the\n",
            "A large language model ( LLM ) is a type of machine learning model designed for natural language processing tasks such as language generation . LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering . [ ] These models acquire predictive power regarding syntax , semantics , and ontologies [ ] inherent in human language corpora, but they also inherit inaccuracies and biases present in the\n",
            "A large language model ( LLM ) is a type of machine learning model designed for natural language processing tasks such as language generation . LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering . [ ] These models acquire predictive power regarding syntax , semantics , and ontologies [ ] inherent in human language corpora, but they also inherit inaccuracies and biases present in the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"user input:\",user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEPP0dISnsFZ",
        "outputId": "c7c4b25c-83d6-4c27-9aa4-d206d7241823"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user input: what is an llm?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.55.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh2gzJPEqQHv",
        "outputId": "fdd2e51a-46ea-41bf-99ab-23bf0e74e005"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.55.3\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.55.3) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (2.27.2)\n",
            "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/389.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.59.4\n",
            "    Uninstalling openai-1.59.4:\n",
            "      Successfully uninstalled openai-1.59.4\n",
            "Successfully installed openai-1.55.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "f = open(\"open-aikey.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "ydcFOmHOqRfJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "gpt_model = \"gpt-4o-mini\"\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_gpt4_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please summarize or elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(text_input)\n",
        "\n",
        "response_time = time.time() - start_time  # Measure response time\n",
        "print(f\"Response Time: {response_time:.2f} seconds\")  # Print response time\n",
        "\n",
        "print(gptmodel,\"Response:\", gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeW8sw5QqgB3",
        "outputId": "6e840109-f49c-4bff-8859-ac9ccbb76f84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Time: 3.36 seconds\n",
            "gpt-4o-mini Response: A large language model (LLM) is a type of machine learning model specifically designed for tasks related to natural language processing (NLP), such as language generation. These models are characterized by having a significant number of parameters and are typically trained using self-supervised learning on extensive datasets of text.\n",
            "\n",
            "The most advanced and capable LLMs are generative pre-trained transformers (GPTs). These models can be fine-tuned for specific tasks or guided by prompt engineering to enhance their performance in particular applications.\n",
            "\n",
            "LLMs acquire predictive capabilities concerning syntax, semantics, and ontologies that are inherent in human language. However, they also inherit inaccuracies and biases that exist within the training data. This duality highlights both the potential and the limitations of LLMs in understanding and generating human language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"GPT-4 Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")\n",
        "\n",
        "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUET0subqjtp",
        "outputId": "b2ca6c9e-7fbc-48ce-8764-4dd44b23ba66"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-4 Response:\n",
            "---------------\n",
            "A large language model (LLM) is a type of machine learning model specifically\n",
            "designed for tasks related to natural language processing (NLP), such as\n",
            "language generation. These models are characterized by having a significant\n",
            "number of parameters and are typically trained using self-supervised learning on\n",
            "extensive datasets of text.  The most advanced and capable LLMs are generative\n",
            "pre-trained transformers (GPTs). These models can be fine-tuned for specific\n",
            "tasks or guided by prompt engineering to enhance their performance in particular\n",
            "applications.  LLMs acquire predictive capabilities concerning syntax,\n",
            "semantics, and ontologies that are inherent in human language. However, they\n",
            "also inherit inaccuracies and biases that exist within the training data. This\n",
            "duality highlights both the potential and the limitations of LLMs in\n",
            "understanding and generating human language.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluator"
      ],
      "metadata": {
        "id": "xMN8T2-1sAgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cosine Similarity"
      ],
      "metadata": {
        "id": "Hf2zSwUwsDXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Example usage with your existing functions\n",
        "similarity_score = calculate_cosine_similarity(text_input, gpt4_response)\n",
        "\n",
        "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_hd7Wrdr9RR",
        "outputId": "308ac424-ee93-4edc-d72a-d8b2134b07de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score: 0.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.Human user rating"
      ],
      "metadata": {
        "id": "Md4buOplsHbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Score parameters\n",
        "counter=20                      # number of queries\n",
        "score_history=60                # human feedback\n",
        "threshold=4                     # minimum rankings to trigger human expert feedback"
      ],
      "metadata": {
        "id": "BbkQcaebsGLC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def evaluate_response(response):\n",
        "    print(\"\\nGenerated Response:\")\n",
        "    print(response)\n",
        "    print(\"\\nPlease evaluate the response based on the following criteria:\")\n",
        "    print(\"1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\")\n",
        "    score = input(\"Enter the relevance and coherence score (1-5): \")\n",
        "    try:\n",
        "        score = int(score)\n",
        "        if 1 <= score <= 5:\n",
        "            return score\n",
        "        else:\n",
        "            print(\"Invalid score. Please enter a number between 1 and 5.\")\n",
        "            return evaluate_response(response)  # Recursive call if the input is invalid\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a numerical value.\")\n",
        "        return evaluate_response(response)  # Recursive call if the input is invalid\n",
        "\n",
        "score = evaluate_response(gpt4_response)\n",
        "print(\"Evaluator Score:\", score)\n",
        "\n",
        "counter+=1\n",
        "score_history+=score\n",
        "mean_score=round(np.mean(score_history/counter), 2)\n",
        "if counter>0:\n",
        "  print(\"Rankings      :\", counter)\n",
        "  print(\"Score history : \", mean_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw2K6eZcsKIS",
        "outputId": "8d1a8aa3-09b5-4647-85a0-2ce035618d18"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "A large language model (LLM) is a type of machine learning model specifically designed for tasks related to natural language processing (NLP), such as language generation. These models are characterized by having a significant number of parameters and are typically trained using self-supervised learning on extensive datasets of text.\n",
            "\n",
            "The most advanced and capable LLMs are generative pre-trained transformers (GPTs). These models can be fine-tuned for specific tasks or guided by prompt engineering to enhance their performance in particular applications.\n",
            "\n",
            "LLMs acquire predictive capabilities concerning syntax, semantics, and ontologies that are inherent in human language. However, they also inherit inaccuracies and biases that exist within the training data. This duality highlights both the potential and the limitations of LLMs in understanding and generating human language.\n",
            "\n",
            "Please evaluate the response based on the following criteria:\n",
            "1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\n",
            "Enter the relevance and coherence score (1-5): 3\n",
            "Evaluator Score: 3\n",
            "Rankings      : 21\n",
            "Score history :  3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## |3.4.Human expert evaluation\n",
        "Human feedback for RAG adaptive loop preparation.\n",
        "\n",
        "Flashcard creation.\n",
        "\n",
        "images for the expert's interface"
      ],
      "metadata": {
        "id": "hSpkRaMZsVO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter_threshold=10\n",
        "score_threshold=4\n",
        "\n",
        "if counter>counter_threshold and score_history<=score_threshold:\n",
        "  print(\"Human expert evaluation is required for the feedback loop.\")"
      ],
      "metadata": {
        "id": "dHwCuNg7sMNq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtEIqbYJsgdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}