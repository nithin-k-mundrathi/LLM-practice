{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+8Ej7ZaDWvRNkxEoUgslK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithin-k-mundrathi/LLM-practice/blob/main/RAG/RAG_C_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "QgTWh3Tq0JJp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/open-aikey.txt\", \"r\")\n",
        "API_KEY=f.readline().strip()\n",
        "f.close()\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "SnHZTxaVx5Gt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3AFBqzsd-UYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o-mini\"\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "maQQsjQix_bG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "d9nKpF9ryM_g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JlkLrx461mhZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll1YWkQv1pFM",
        "outputId": "12c0843b-8bac-4519-bbc9-75c537a26c17"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "7HXLG3Cm1qps"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN5VpCJN1tWN",
        "outputId": "f386763b-f016-40bd-c54b-42929a1e3bed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Error code: 401 - {'error': {'message': \"Incorrect API key provided: 'sk-proj***\n",
            "********************************************************************************\n",
            "***********************************************************************8YA'. You\n",
            "can find your API key at https://platform.openai.com/account/api-keys.\", 'type':\n",
            "'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        use_idf=True,\n",
        "        norm='l2',\n",
        "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
        "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
        "    )\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]"
      ],
      "metadata": {
        "id": "bHsyOhwA1uxQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmatized_words = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        lemmatized_words.append(token.lemma_)\n",
        "    return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "    expanded_words = words.copy()\n",
        "    for word in words:\n",
        "        expanded_words.extend(get_synonyms(word))\n",
        "    return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRXsOwg23sQx",
        "outputId": "6b4cfb19-4fef-47e6-8cfd-cd17909ccd50"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsWC5R9E3zIi",
        "outputId": "7e0f3cf9-e9f3-4790-d083-41ccaa550ba3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 3\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cosine Similarity\n",
        "score = calculate_cosine_similarity(query, best_matching_record)\n",
        "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCbpz2dv4H7Y",
        "outputId": "3e4dc33f-61a3-4e70-eecc-adcac75a95d0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fp2nx7d4Lqr",
        "outputId": "7636ad2c-d7d3-4333-c8fe-be805c397186"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+ \": \"+ best_matching_record"
      ],
      "metadata": {
        "id": "jsuiMBH_4NTT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtvVx4xV4VXI",
        "outputId": "b6c9a705-974b-4e7b-fac4-71d4ad77d4c2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQBXnl234WGG",
        "outputId": "e588ae6a-887b-4219-f49e-6d96a5bcbe82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A **RAG vector store** refers to a specialized database or dataset designed to\n",
            "store and manage vectorized data points, particularly in the context of\n",
            "retrieval-augmented generation (RAG) models in natural language processing\n",
            "(NLP).   ### Key Components of a RAG Vector Store:  1. **Vectorization**:    -\n",
            "Vectorization is the process of converting data (such as text, images, or other\n",
            "forms of information) into numerical vectors. This transformation allows the\n",
            "data to be represented in a format that machine learning models can process. In\n",
            "NLP, this often involves using techniques like word embeddings (e.g., Word2Vec,\n",
            "GloVe) or transformer-based embeddings (e.g., BERT, GPT).  2. **Data Points**:\n",
            "- The vector store contains numerous data points, each represented as a vector.\n",
            "These data points can be derived from various sources, such as documents,\n",
            "sentences, or even individual words. Each vector captures the semantic meaning\n",
            "of the corresponding data point in a high-dimensional space.  3. **Retrieval\n",
            "Mechanism**:    - One of the primary functions of a RAG vector store is to\n",
            "facilitate efficient retrieval of relevant vectors based on a query. When a user\n",
            "inputs a query, the system can quickly search through the stored vectors to find\n",
            "those that are most similar or relevant to the query. This is often achieved\n",
            "using techniques like nearest neighbor search or similarity measures (e.g.,\n",
            "cosine similarity).  4. **Augmentation**:    - The term \"augmented\" in RAG\n",
            "refers to the process of enhancing the generative capabilities of a model by\n",
            "incorporating external information. By retrieving relevant data points from the\n",
            "vector store, the model can generate more informed and contextually appropriate\n",
            "responses. This is particularly useful in tasks like question answering, where\n",
            "the model can pull in specific information to answer queries accurately.  5.\n",
            "**Applications**:    - RAG vector stores are used in various applications,\n",
            "including chatbots, information retrieval systems, and content generation tools.\n",
            "They enable systems to provide more accurate and context-aware responses by\n",
            "leveraging a rich set of stored knowledge.  ### Conclusion: In summary, a RAG\n",
            "vector store is a crucial component in modern NLP systems, allowing for the\n",
            "efficient storage, retrieval, and utilization of vectorized data points. By\n",
            "integrating these capabilities, RAG models can enhance their performance in\n",
            "generating relevant and contextually rich outputs.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## advanced RAG"
      ],
      "metadata": {
        "id": "23PBVwPT5Pvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the TFIDF vectors here\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record"
      ],
      "metadata": {
        "id": "wgHeOj3_4e6N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
      ],
      "metadata": {
        "id": "-_ZCerlS5VOD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDCCBLuk5txp",
        "outputId": "c5dc93e0-0b67-44e6-99c8-e7221c3141f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEM-Sokl5wlP",
        "outputId": "b0efac52-86cd-460e-a534-5460396123e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqREhW5y5zQk",
        "outputId": "b1362e54-6be3-486a-e920-0337523d0673"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ],
      "metadata": {
        "id": "s5dbaErt51fq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg9at9J_50dH",
        "outputId": "f80577c5-2a04-4f66-d890-c61792ab99e0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAOXBrQq54D3",
        "outputId": "7785c16f-8515-4cb5-94c2-c604223e9d2d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A **RAG vector store** refers to a specialized database or dataset designed to\n",
            "hold vectorized data points, particularly in the context of retrieval-augmented\n",
            "generation (RAG) models in natural language processing (NLP). Let's break down\n",
            "the concept further:  ### What is a Vector Store?  1. **Vectorization**: In NLP\n",
            "and machine learning, vectorization is the process of converting data (such as\n",
            "text, images, or other forms of information) into numerical vectors. These\n",
            "vectors are typically high-dimensional arrays that represent the features of the\n",
            "data in a way that can be processed by algorithms.  2. **Data Points**: A vector\n",
            "store contains multiple data points, each represented as a vector. For example,\n",
            "in the case of text data, each document or sentence can be transformed into a\n",
            "vector using techniques like word embeddings (e.g., Word2Vec, GloVe) or sentence\n",
            "embeddings (e.g., BERT, Sentence Transformers).  3. **Storage and Retrieval**:\n",
            "The primary function of a vector store is to efficiently store these vectors and\n",
            "enable quick retrieval based on similarity. This is crucial for applications\n",
            "like information retrieval, recommendation systems, and conversational agents,\n",
            "where finding relevant data points quickly is essential.  ### RAG (Retrieval-\n",
            "Augmented Generation)  1. **Retrieval-Augmented Generation**: RAG is a framework\n",
            "that combines retrieval-based methods with generative models. It enhances the\n",
            "capabilities of generative models (like GPT) by allowing them to access external\n",
            "knowledge bases or datasets during the generation process.  2. **How RAG\n",
            "Works**: In a typical RAG setup, when a query is made, the system first\n",
            "retrieves relevant documents or data points from the vector store based on the\n",
            "input query. These retrieved vectors are then used as context for the generative\n",
            "model to produce a more informed and contextually relevant response.  ###\n",
            "Applications of RAG Vector Stores  1. **Question Answering**: RAG vector stores\n",
            "can be used to improve question-answering systems by retrieving relevant\n",
            "documents that contain answers to user queries.  2. **Chatbots and\n",
            "Conversational Agents**: By accessing a vector store of previous conversations\n",
            "or knowledge bases, chatbots can provide more accurate and context-aware\n",
            "responses.  3. **Content Recommendation**: In recommendation systems, vector\n",
            "stores can help suggest content (articles, videos, etc.) that is similar to what\n",
            "a user has previously engaged with.  ### Conclusion  In summary, a RAG vector\n",
            "store is a crucial component in modern NLP systems that utilize retrieval-\n",
            "augmented generation techniques. It enables the efficient storage and retrieval\n",
            "of vectorized data points, enhancing the performance and relevance of generative\n",
            "models in various applications. By leveraging the power of vectorization and\n",
            "retrieval, RAG systems can provide more accurate and contextually rich outputs.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index based Search"
      ],
      "metadata": {
        "id": "C4i-APTE5_WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgzfHQGv56V3",
        "outputId": "8a857be2-6507-4aed-eef7-f96f51c61f2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZBRcHK_6OM-",
        "outputId": "0c96c376-d947-465e-f6fb-2580102c880b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2WqPEAS6QRU",
        "outputId": "3e419a87-842b-4f60-f6cc-b2d27a71a050"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ],
      "metadata": {
        "id": "LUhD122P6R6A"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LUvT5y36Z9a",
        "outputId": "d3b0a0f9-64d7-41ac-e3fc-f17e0978336a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgXcOWg_6bYt",
        "outputId": "3b11c118-8c5d-437b-872b-8d90c2dd8bf6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A **RAG vector store** refers to a specialized database or dataset designed to\n",
            "store and manage vectorized data points, particularly in the context of\n",
            "Retrieval-Augmented Generation (RAG) models in Natural Language Processing\n",
            "(NLP). Hereâ€™s a detailed breakdown of the concept:  ### What is a Vector Store?\n",
            "1. **Vector Representation**: In machine learning and NLP, data points (such as\n",
            "words, sentences, or documents) are often represented as vectors. A vector is a\n",
            "mathematical representation that captures the semantic meaning of the data in a\n",
            "multi-dimensional space. For example, word embeddings like Word2Vec or sentence\n",
            "embeddings from models like BERT convert textual data into numerical vectors.\n",
            "2. **Storage and Retrieval**: A vector store is designed to efficiently store\n",
            "these vectors and facilitate quick retrieval based on similarity. This is\n",
            "crucial for applications like search engines, recommendation systems, and\n",
            "conversational agents, where the ability to find relevant information quickly is\n",
            "essential.  ### RAG (Retrieval-Augmented Generation)  1. **Retrieval-Augmented\n",
            "Generation**: RAG is a framework that combines retrieval-based methods with\n",
            "generative models. It enhances the capabilities of generative models (like GPT)\n",
            "by allowing them to pull in relevant information from a database or knowledge\n",
            "base during the generation process. This helps in producing more accurate and\n",
            "contextually relevant responses.  2. **Role of Vector Store in RAG**: In a RAG\n",
            "setup, the vector store plays a critical role. When a query is made, the system\n",
            "retrieves relevant vectors from the vector store that correspond to the query.\n",
            "These vectors represent pieces of information that can be used to inform the\n",
            "generative model, allowing it to produce responses that are not only coherent\n",
            "but also grounded in factual data.  ### Key Features of a RAG Vector Store  -\n",
            "**Scalability**: The vector store must be able to handle large volumes of data,\n",
            "as modern applications often require processing millions of data points.    -\n",
            "**Efficiency**: It should support fast similarity searches, often using\n",
            "techniques like approximate nearest neighbor (ANN) search to quickly find the\n",
            "most relevant vectors.  - **Flexibility**: The store should accommodate various\n",
            "types of data, including text, images, and other modalities, depending on the\n",
            "application.  - **Integration**: It should seamlessly integrate with both the\n",
            "retrieval and generation components of the RAG framework, allowing for smooth\n",
            "data flow and interaction.  ### Applications  - **Chatbots and Virtual\n",
            "Assistants**: RAG vector stores can enhance the performance of chatbots by\n",
            "providing them with access to a vast amount of information, enabling them to\n",
            "answer questions more accurately.  - **Search Engines**: They can improve search\n",
            "results by retrieving relevant documents or snippets based on user queries and\n",
            "generating summaries or answers.  - **Content Creation**: In content generation\n",
            "tools, RAG vector stores can provide contextually relevant information that\n",
            "helps in creating high-quality content.  In summary, a RAG vector store is a\n",
            "crucial component in modern NLP systems, enabling efficient storage, retrieval,\n",
            "and utilization of vectorized data to enhance the performance of generative\n",
            "models through the integration of relevant information.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular RAG"
      ],
      "metadata": {
        "id": "6wTrsg9N6eKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "      self.documents = records  # Initialize self.documents here\n",
        "      if self.method == 'vector' or self.method == 'indexed':\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ],
      "metadata": {
        "id": "NdHDdac76c0Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1ZFFw_R9szs",
        "outputId": "3ff4f1d9-ec13-4c0f-dd7c-c985eb88ab99"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdOEJN1T9vu0",
        "outputId": "bb713495-8d13-429d-f65e-c8de0391d0cb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(\"Enhanced Similarity:\", similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzOY_aAl9xjM",
        "outputId": "404891a4-a54a-41d8-c41d-63af626020ba"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity: 0.641582812483307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+ \" \"+ best_matching_record"
      ],
      "metadata": {
        "id": "Zo6yJSqA9y4U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7UVfZKC90KO",
        "outputId": "5502feb6-e7db-42f2-de69-eb22eccad802"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nljtAMgd91qy",
        "outputId": "1fd7560a-406d-4226-d030-82fad64429d5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A **RAG vector store** refers to a specialized database or dataset designed to\n",
            "store and manage vectorized data points, particularly in the context of machine\n",
            "learning and natural language processing (NLP). The term \"RAG\" typically stands\n",
            "for \"Retrieval-Augmented Generation,\" which is a framework that combines\n",
            "retrieval-based methods with generative models to enhance the performance of\n",
            "tasks such as question answering, summarization, and other NLP applications.\n",
            "### Key Components of a RAG Vector Store:  1. **Vectorization**:    -\n",
            "Vectorization is the process of converting data (such as text, images, or other\n",
            "forms of information) into numerical vectors. This transformation allows the\n",
            "data to be processed by machine learning algorithms, which often require\n",
            "numerical input.    - In NLP, text data is commonly vectorized using techniques\n",
            "like Word2Vec, GloVe, or more advanced methods like BERT and Sentence\n",
            "Transformers, which create dense representations of words or sentences in a\n",
            "high-dimensional space.  2. **Data Points**:    - The vector store contains\n",
            "numerous data points, each represented as a vector. These data points can\n",
            "represent various entities, such as sentences, paragraphs, or entire documents,\n",
            "depending on the application.    - Each vector captures semantic information\n",
            "about the corresponding data point, allowing for meaningful comparisons and\n",
            "retrieval based on similarity.  3. **Storage and Retrieval**:    - A RAG vector\n",
            "store is optimized for efficient storage and retrieval of these vectors. It\n",
            "often employs data structures like KD-trees, Ball trees, or more advanced\n",
            "indexing techniques to facilitate quick nearest neighbor searches.    - This\n",
            "capability is crucial for applications where rapid access to relevant\n",
            "information is needed, such as in real-time question-answering systems.  4.\n",
            "**Integration with Generative Models**:    - In the RAG framework, the vector\n",
            "store is used in conjunction with generative models. When a query is made, the\n",
            "system retrieves relevant vectors from the store based on their similarity to\n",
            "the query vector.    - The retrieved vectors can then be used as context or\n",
            "input for a generative model, which produces a coherent and contextually\n",
            "relevant response or output.  5. **Applications**:    - RAG vector stores are\n",
            "particularly useful in scenarios where large amounts of unstructured data need\n",
            "to be processed and understood. Common applications include:      - Chatbots and\n",
            "virtual assistants that provide information based on user queries.      -\n",
            "Document retrieval systems that find relevant documents based on a search query.\n",
            "- Content generation tools that create summaries or articles based on retrieved\n",
            "information.  ### Conclusion: In summary, a RAG vector store is a crucial\n",
            "component in modern NLP systems, enabling efficient storage, retrieval, and\n",
            "processing of vectorized data points. By leveraging the power of vector\n",
            "representations and combining them with generative models, RAG frameworks\n",
            "enhance the ability to understand and generate human-like text, making them\n",
            "valuable in various applications across industries.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8614wN8o93gC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}